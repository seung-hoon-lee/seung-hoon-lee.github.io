---
---

@string{aps = {American Physical Society,}}


@inproceedings{lee2025accuquant,
  bibtex_show={true},
  title={AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models},
  author={Lee*, Seunghoon and Choi*, Jeongwoo and Son, Byunggwan and Moon, Jaehyeon and Jeon, Jeimin and Ham, Bumsub},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025},
  html={/404.html},
  selected={true},
  preview={accuquant_overview.png},
  abbr={NeurIPS (poster)},
  abstract={We present in this paper a novel post-training quantization (PTQ) method, dubbed AccuQuant, for diffusion models. We show analytically and empirically that quan3 tization errors for diffusion models are accumulated over denoising steps in a sampling process. To alleviate the error accumulation problem, AccuQuant mini5 mizes the discrepancies between outputs of a full-precision diffusion model and its quantized version within a couple of denoising steps. That is, it simulates multiple denoising steps of a diffusion sampling process explicitly for quantization, account8 ing the accumulated errors over multiple denoising steps, which is in contrast to previous approaches to imitating a training process of diffusion models, namely, minimizing the discrepancies independently for each step. We also present an efficient implementation technique for AccuQuant, together with a novel objective, which reduces a memory complexity significantly from O(n) to O(1), where n is the number of denoising steps. We demonstrate the efficacy and efficiency of AccuQuant across various tasks and diffusion models on standard benchmarks.}
}

@misc{lee2026qsam,
  bibtex_show={false},
  title={Q-SAM: Quantizing Segment Anything Models},
  author={Lee, Seunghoon and Choi, Jeongwoo and Seo, Yura and Ham, Bumsub},
  booktitle={under review},
  year={2025},
  html={/404.html},
  selected={true},
  preview={qsam_overview.png},
  abbr={under review},
  abstract={},
}

@inproceedings{lee2022uda,
  bibtex_show={false},
  title={Unsupervised Domain Adaptation Solving Class-Imbalance Problem on Semantic Segmentation},
  author={Lee*, Seunghoon and Cho*, Donghyeon and Han, Dongil},
  booktitle={3rd Korea Artificial Intelligence Conference},
  year={2022},
  html={/404.html},
  selected={true},
  preview={uda_overview.png},
  abbr={KoreaAI},
  abstract={Deep learning models, which have been intensively studied in recent years, require a large quantity of data and paired labels in order to perform well. However, manually obtaining and labeling substantial amounts of data is quite expensive. In addition, even if the model is trained with such collected data, the model may not operate as expected due to domain shift, and it is difficult to expect good performance for the class if the amount of data collected per class is insufficient. In this paper, we propose applying a target domain style to the source domain, expecting the trained model without a target label to operate smoothly in the target domain, and using the Likely Location copy-paste method to solve the class imbalance problem for minor classes.}
}